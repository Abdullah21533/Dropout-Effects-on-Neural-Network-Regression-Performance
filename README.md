# ğŸ“˜ Dropout Effects on Neural Network Regression Performance

This repository presents a clean, structured implementation of a comparative study between two feedforward neural network models â€” one with dropout regularization and one without â€” trained on synthetic regression data. The goal is to demonstrate the effect of dropout on overfitting and generalization in a regression setting.

---

## ğŸ” Project Summary

While dropout is a well-known regularization technique in classification, its application in regression tasks is less explored. This project aims to evaluate how dropout impacts model performance using a small, noisy dataset.

Two models are trained and evaluated:

- ğŸ”¹ **Baseline Model**: Two fully connected layers without dropout  
- ğŸ”¸ **Dropout Model**: Same architecture with `Dropout(0.2)` after each hidden layer

---

## ğŸ§  Key Features

- Synthetic 1D regression dataset
- Feedforward neural network implementation using TensorFlow/Keras
- Comparison of training vs. validation MSE
- Visual analysis including:
  - ğŸ“Š Data distribution
  - ğŸ“‰ Loss curves
  - ğŸ” Predicted vs. true outputs

---

## ğŸ§ª Dataset Overview

| Set     | Size | Description                          |
|---------|------|--------------------------------------|
| Train   | 20   | Synthetic X values from -1 to 1 with corresponding noisy targets |
| Test    | 20   | Independent synthetic evaluation data in same range |

---

## âš™ï¸ Model Architectures

### ğŸŸ¦ Baseline Model (No Dropout)
- Dense(128, activation='relu')
- Dense(128, activation='relu')
- Dense(1, activation='linear')

### ğŸŸ§ Dropout Model
- Dense(128, activation='relu')
- Dropout(0.2)
- Dense(128, activation='relu')
- Dropout(0.2)
- Dense(1, activation='linear')

> Optimizer: `Adam` (learning rate = 0.01)  
> Loss Function: `Mean Squared Error (MSE)`  
> Epochs: 500

---

## ğŸ“ˆ Results Snapshot

| Model           | Train MSE | Test MSE | Overfitting |
|----------------|-----------|----------|-------------|
| No Dropout     | Lower     | Higher   | âš ï¸ Likely   |
| With Dropout   | Slightly Higher | Lower | âœ… Reduced |

> âœ… Dropout helps improve generalization on unseen data, especially in small-data scenarios.

---

## ğŸ“Š Visualizations

- ğŸ”´ **Data Distribution Plot**: Shows the synthetic train/test sets
- ğŸ“‰ **Training vs. Validation Loss**: Over 500 epochs
- ğŸ” **Model Predictions**: True values vs. predicted values for both models

---

## ğŸ› ï¸ Technologies Used

- Python 3.x  
- TensorFlow / Keras  
- NumPy  
- Matplotlib  

---

## ğŸ“ Project Structure
ğŸ“¦ dropout-effects-regression/
â”œâ”€â”€ dropout_comparison_clean.py
â”œâ”€â”€ figures/
â”‚ â”œâ”€â”€ data_distribution.png
â”‚ â”œâ”€â”€ prediction_comparison.png
â”‚ â””â”€â”€ training_loss_curves.png
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
